{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab5a51be",
   "metadata": {},
   "source": [
    "# <font color = teal>  Introduction to testing and evaluating models </font>\n",
    "\n",
    "Testing phase can be performed either with a single yaml file or with several yaml files located in the same directory.\n",
    "\n",
    "To make predictions with a trained model, you'll need a csv file to tell a model which parts of the data (i.e., which ECGs) are used as a testing set, and a yaml file which names this csv file for the model. The csv file(s) can be created by following the introductions in the notebook [Introduction to data handling](1_introduction_data_handling.ipynb). Yaml files can be created with the notebooks [Yaml files of database-wise split for training and testing](2_physionet_DBwise_yaml_files.ipynb) and [Yaml files of stratified split for training and testing](2_physionet_stratified_yaml_files.ipynb). And of course, you should have trained a model to test it: To do this, you can find some introduction in the notebook [Introduction to training models](3_introduction_training.ipynb). \n",
    "\n",
    "-----------------\n",
    "\n",
    "<font color ='red'> **NOTE!** </font> *Before you start testing the models, especially when you have made predictions multiple times, check the saving directory that it either contains the predictions you have made in the previous iteration or is empty. If you use different test data with the same yaml file, you might end up having predictions from different csv files and evaluation doesn't work. Mind this especially, if you get an **AssertionError**.*\n",
    "\n",
    "As with the training phase, yaml files are also used in the prediction and evaluation phase. They have a structure as follows (`predict_smoke.yaml`)\n",
    "\n",
    "```\n",
    "# INITIAL SETTINGS\n",
    "test_file: test_split_1.csv\n",
    "model: split_1_1.pth\n",
    "\n",
    "# TESTING SETTINGS\n",
    "threshold: 0.500000\n",
    "\n",
    "# DEVICE CONFIGS\n",
    "device_count: 1\n",
    "\n",
    "```\n",
    "\n",
    "where `test_file` refers to the csv file of the test data and `model` refers to the file of the trained model which you want to test. The testing settings include decision threshold. \n",
    "\n",
    "<font color ='red'>**NOTE!**</font> The `device_count` attribute should be considered. It refers to the number of GPUs which are used in prediction, similarly as in the training phase.\n",
    "\n",
    "The script for this phase is `run_model.py`. You should first check the path `csv_root` that it points to the right location in the `data` directory (and its subdirectories). The attribute is set to find the csv file of the testing data. Note that as it's concatenated with the `os.path.join` method, you may only need to change the last path component. `model` will be searched from the `experiments` directory automatically so only the name of the file is nessessary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeff5f78",
   "metadata": {},
   "source": [
    "### <font color = teal> Saving the results </font>\n",
    "\n",
    "The predictions are saved as csv files with the following structure\n",
    "\n",
    "```\n",
    "#Record ID\n",
    "164889003, 270492004, 164909002, 426783006, 59118001, 284470004,  164884008,\n",
    "        1,         1,         0,         0,         0,        0,          0,        \n",
    "      0.9,       0.6,       0.2,       0.05,      0.2,      0.35,       0.35,  \n",
    "```\n",
    "\n",
    "where the first row, `#Record ID`, refers to the file name from which the prediction is made, and the second to the class labels used in SNOMED CT codes, and the third row to the predicted label in binary form (1 - patient is predicted to have the diagnosis above, and 0 - the opposite), and the fourth row to the probability scores for each predicted label. \n",
    "\n",
    "The script automatically performs the evaluation automatically after the predictions are made. In the evaluation phase, `metrics.py` is used from `/src/modeling/` to compute the wanted metrics. The evaluation metrics will be saved in the same directory as the predictions as a `pickle` file. ROC curves are also drawn as previously, and they are saved in a `png` format. \n",
    "\n",
    "By default, evaluation history will contain micro/macro AUROC scores and micro/macro average precision scores as well as Physionet Challenge 2021 scoring metric (more information about this scoring metric can be found [here under **Scoring**](https://moody-challenge.physionet.org/2021/)). Also, decision threshold, labels and path for the test csv file are saved. These can be modified in the `predict_utils` script in `./src/modeling/` if wanted. There are a `history` dictionary located in the first lines of the `predict` function.\n",
    "\n",
    "If other metrics are wanted to be computed, you can compute them in the `metrics` scripts in `./src/modeling/`. There can be found the `cal_multilabel_metrics` function, in which the wanted metrics are computed and where you can add new functions. The predicted labels are also one-hot-encoded there, so this format of the predictions is available."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e87a9d",
   "metadata": {},
   "source": [
    "### <font color = teal> Terminal commands </font>\n",
    "\n",
    "Run a terminal command which consist of the script and the yaml file *or* the directory where all the yaml files are located, so one of the followings\n",
    "\n",
    "```\n",
    "python run_model.py predict_smoke.yaml\n",
    "python run_model.py predict_stratified_smoke\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "349e3950",
   "metadata": {},
   "source": [
    "---------------------\n",
    "\n",
    "## <font color = teal> Example: Smoke testing </font>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7ba3594d",
   "metadata": {},
   "source": [
    "<font color = red>**NOTE!**</font> <font color = green> **Here, the assumption is that *the data is preprocessed*. If that's not the case, you should use, for example, the original data directory, such as the** `smoke_data` **directory.** The paths for ECGs will be different in the csv files depending on whether preprocessing has been used or not.</font>\n",
    "\n",
    "### <font color = teal> One yaml file </font>\n",
    "\n",
    "The yaml file for smoke testing --- `predict_smoke.yaml` --- is available in `/configs/predicting/`. Make sure that the model is trained first and is named as `train_smoke.pth`.\n",
    "\n",
    "*And before anything, check if there exists a directory named `predict_smoke` in the `experiments` directory. If there are other predictions made and they are not the ones from the files listed below, evaluation won't work correctly. Mind this especially when you get an **AssertionError**.*\n",
    "\n",
    "The csv file `test_split_1.csv` has the following structure\n",
    "\n",
    "```\n",
    "path,age,gender,fs,426783006,426177001,164934002,427084000,164890007,39732003,164889003,59931005,427393009,270492004\n",
    "./data/physionet_preprocessed_smoke/CPSC_CPSC-Extra/A0004_preprocessed.mat,45.0,Male,500.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0\n",
    "./data/physionet_preprocessed_smoke/CPSC_CPSC-Extra/A0003_preprocessed.mat,81.0,Female,500.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0\n",
    "./data/physionet_preprocessed_smoke/CPSC_CPSC-Extra/A0007_preprocessed.mat,74.0,Male,500.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0\n",
    "./data/physionet_preprocessed_smoke/CPSC_CPSC-Extra/Q0001_preprocessed.mat,53.0,Male,500.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0\n",
    "./data/physionet_preprocessed_smoke/CPSC_CPSC-Extra/A0009_preprocessed.mat,81.0,Male,500.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0\n",
    "./data/physionet_preprocessed_smoke/CPSC_CPSC-Extra/A0002_preprocessed.mat,49.0,Female,500.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0\n",
    "```\n",
    "\n",
    "so total of six files are considered as the testing data. All of them are from the CPSC and CPSC-Extra databases.\n",
    "\n",
    "The path in `run_model.py` should be set as below (note the last path component):\n",
    "\n",
    "```\n",
    "csv_root = os.path.join('data', 'split_csvs', 'stratified_smoke')\n",
    "```\n",
    "\n",
    "Then you can just run the command to make the predictions with the trained model saved as `train_smoke.pth` as\n",
    "\n",
    "```\n",
    "python run_model.py predict_smoke.yaml\n",
    "```\n",
    "\n",
    "The predictions can be found in the `predict_smoke` subdirectory of the `experiments` directory. Each prediction is named after the original file name from which the predictions have been made. In smoke testing, they are as follows\n",
    "\n",
    "```\n",
    "A0002_preprocessed.csv\n",
    "A0003_preprocessed.csv\n",
    "A0004_preprocessed.csv\n",
    "A0007_preprocessed.csv\n",
    "A0009_preprocessed.csv\n",
    "Q0001_preprocessed.csv\n",
    "```\n",
    "\n",
    "Each have the structure of the one presented above. \n",
    "\n",
    "Wanted metrics (micro and macro average precisions and micro and macro AUROCs) are saved in the file `test_history.pickle` and can be found in the same directory as the predictions are located. The ROC curve is saved as `roc-test.png`.\n",
    "\n",
    "\n",
    "### <font color = teal> Multiple yaml files in a directory </font>\n",
    "\n",
    "The idea is similar here: Now you should locate all the yaml files in one directory. The `predict_stratified_smoke` directory in `/configs/predicting/` was created when the yaml files were created with the notebook [Yaml files of stratified split for training and testing](2_physionet_stratified_yaml_files.ipynb). There are four yaml files named as `split_1_1.yaml`, `split_1_2.yaml`, `split_1_3.yaml` and `split_1_4.yaml` in the directory. Each yaml file has the same test file as they've been created using the same stratificated data split, but they do have different models to use for the testing phase. For example, the two first files has the following content:\n",
    "\n",
    "`split_1_1.yaml`:\n",
    "```\n",
    "# INITIAL SETTINGS\n",
    "test_file: test_split_1.csv\n",
    "model: split_1_1.pth\n",
    "\n",
    "# TESTING SETTINGS\n",
    "threshold: 0.500000\n",
    "\n",
    "# DEVICE CONFIGS\n",
    "device_count: 1\n",
    "\n",
    "```\n",
    "\n",
    "`split_1_2.yaml`:\n",
    "```\n",
    "# INITIAL SETTINGS\n",
    "test_file: test_split_1.csv\n",
    "model: split_1_2.pth\n",
    "\n",
    "# TESTING SETTINGS\n",
    "threshold: 0.500000\n",
    "\n",
    "# DEVICE CONFIGS\n",
    "device_count: 1\n",
    "```\n",
    "\n",
    "The path in `run_model.py` should be set as below (note the last path component):\n",
    "\n",
    "```\n",
    "csv_root = os.path.join('data', 'split_csvs', 'stratified_smoke')\n",
    "```\n",
    "\n",
    "Terminal command for testing is now\n",
    "\n",
    "```\n",
    "python run_model.py predict_stratified_smoke\n",
    "```\n",
    "\n",
    "The predictions can be found from two subdirectories of the `predict_stratified_smoke` directory in `/experiments/` named as used yaml files. Also the evaluation metrics are saved in both subdirectories in `pickle` format, and the ROC curves are saved as `png` pictures."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ecg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "af433470baea3cbfb1d2a9219a544bb72a17c8a5091280fdb93be39946c5da4b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
