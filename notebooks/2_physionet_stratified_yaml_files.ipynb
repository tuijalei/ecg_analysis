{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dca53a29",
   "metadata": {},
   "source": [
    "# <font color = teal> Yaml files of stratified split for training and testing </font>\n",
    "\n",
    "With this notebook, you can create the yaml files needed in training and testing with a data split which is made  using **stratification**. More detailed information about the stratified data split itself is available in the notebook [Introduction to data handling](1_introduction_data_handling.ipynb).\n",
    "\n",
    "------\n",
    "\n",
    "Note that the hyperparameters considering training and testing are set in the yaml files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "10c74753",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Parameters for the yaml files -------------\n",
    "# Training parameters\n",
    "batch_size = 10\n",
    "num_workers = 0\n",
    "epochs = 1\n",
    "lr = 0.003\n",
    "weight_decay = 0.00001\n",
    "\n",
    "# Device configurations\n",
    "device_count = 1\n",
    "\n",
    "# -----------\n",
    "# Decision threshold for predictions\n",
    "threshold = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa3650ed",
   "metadata": {},
   "source": [
    "Examples of the training and the testing yaml files are provided below.\n",
    "\n",
    "**Yaml file for training a model**\n",
    "```\n",
    "# INITIAL SETTINGS\n",
    "train_file: train_split_1_1.csv\n",
    "val_file: val_split_1_1.csv\n",
    "\n",
    "# TRAINING SETTINGS\n",
    "batch_size: 10\n",
    "num_workers: 0\n",
    "epochs: 1\n",
    "lr: 0.003000\n",
    "weight_decay: 0.000010\n",
    "\n",
    "# VALIDATION SETTINGS\n",
    "threshold = 0.5\n",
    "\n",
    "# DEVICE CONFIGS\n",
    "device_count: 1\n",
    "\n",
    "```\n",
    "\n",
    "**Yaml file for testing a model**\n",
    "\n",
    "```\n",
    "# INITIAL SETTINGS\n",
    "test_file: test_split_1.csv\n",
    "model: split_1_1.pth\n",
    "\n",
    "# TESTING SETTINGS\n",
    "threshold: 0.500000\n",
    "\n",
    "# DEVICE CONFIGS\n",
    "device_count: 1\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b88b00d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re\n",
    "from pathlib import Path\n",
    "from ruamel.yaml import YAML\n",
    "  \n",
    "\n",
    "# Absolute path of this file\n",
    "abs_path = Path(os.path.abspath(''))\n",
    "\n",
    "# PARAMETERS TO CREATE STRATIFIED YAML FILES  \n",
    "# ------------------------------------------\n",
    "\n",
    "# From where to load the csv files of stratified split\n",
    "csv_path = os.path.join(abs_path.parent.absolute(), 'data', 'split_csvs', 'stratified_smoke')\n",
    "\n",
    "# Where to save the training yaml files\n",
    "train_yaml_save_path = os.path.join(abs_path.parent.absolute(), 'configs', 'training', 'train_stratified_smoke')\n",
    "\n",
    "# Where to save the testing yaml files\n",
    "test_yaml_save_path = os.path.join(abs_path.parent.absolute(), 'configs', 'predicting', 'predict_stratified_smoke')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e256917d",
   "metadata": {},
   "source": [
    "The directory of the csv files of the stratified data split should be found from `/data/split_csvs/`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c57b038",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_split_1.csv\n",
      "train_split_1_1.csv\n",
      "train_split_1_2.csv\n",
      "train_split_1_3.csv\n",
      "train_split_1_4.csv\n",
      "val_split_1_1.csv\n",
      "val_split_1_2.csv\n",
      "val_split_1_3.csv\n",
      "val_split_1_4.csv\n"
     ]
    }
   ],
   "source": [
    "# Stratified csv files\n",
    "csv_files = sorted([file for file in os.listdir(csv_path) if not file.startswith('.') and file.endswith('.csv')])\n",
    "\n",
    "print(*csv_files, sep='\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3d58cc57",
   "metadata": {},
   "source": [
    "## <font color = teal> Combinations of training and testing sets </font>\n",
    "\n",
    "With stratified split, there are **five** different possibilities to split the Physionet 2021 databases into training and testing sets:\n",
    "\n",
    "    1) train: PTB_PTBXL.csv, INCART.csv, G12EC.csv, ChapmanShaoxing_Ningbo.csv\n",
    "       test: CPSC_CPSC-Extra.csv\n",
    "       \n",
    "    1) train: PTB_PTBXL.csv, INCART.csv, G12EC.csv, CPSC_CPSC-Extra.csv\n",
    "       test: ChapmanShaoxing_Ningbo.csv\n",
    "\n",
    "    2) train: PTB_PTBXL.csv, INCART.csv, ChapmanShaoxing_Ningbo.csv, CPSC_CPSC-Extra.csv\n",
    "       test: G12EC.csv\n",
    "\n",
    "    3) train: PTB_PTBXL.csv, G12EC.csv, ChapmanShaoxing_Ningbo.csv, CPSC_CPSC-Extra.csv\n",
    "       test: INCART.csv\n",
    "\n",
    "    4) train: G12EC.csv, INCART.csv, ChapmanShaoxing_Ningbo.csv, CPSC_CPSC-Extra.csv\n",
    "       test: PTB_PTBXL.csv\n",
    "       \n",
    "Training sets will be stratified into training and validation sets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5773a438",
   "metadata": {},
   "source": [
    "Let's combine the csv files of training, validation and testing sets, e.g., `train_split_1_1.csv`, `val_split_1_1.csv` and `test_split_1.csv`, into a list of lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b0dedbb0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 training and validation pairs\n",
      "('train_split_1_1.csv', 'val_split_1_1.csv')\n",
      "('train_split_1_2.csv', 'val_split_1_2.csv')\n",
      "('train_split_1_3.csv', 'val_split_1_3.csv')\n",
      "('train_split_1_4.csv', 'val_split_1_4.csv')\n",
      "\n",
      "Training, validation and testing pairs\n",
      "['train_split_1_1.csv', 'val_split_1_1.csv', 'test_split_1.csv']\n",
      "['train_split_1_2.csv', 'val_split_1_2.csv', 'test_split_1.csv']\n",
      "['train_split_1_3.csv', 'val_split_1_3.csv', 'test_split_1.csv']\n",
      "['train_split_1_4.csv', 'val_split_1_4.csv', 'test_split_1.csv']\n",
      "\n",
      "Total of 4 training, validation and testing sets\n"
     ]
    }
   ],
   "source": [
    "# First, divide train and validation splits into own lists\n",
    "train_files = [file for file in csv_files if 'train' in file]\n",
    "val_files = [file for file in csv_files if 'val' in file]\n",
    "\n",
    "# Zip these two and convert to list since they should be sorted similarly\n",
    "train_val_pair = list(zip(train_files, val_files))\n",
    "print('First 5 training and validation pairs')\n",
    "print(*train_val_pair[:5], sep='\\n')\n",
    "print()\n",
    "\n",
    "# Seems right based on the print:\n",
    "# Add the prediction fi\n",
    "test_files = [file for file in csv_files if 'test' in file]\n",
    "\n",
    "split_nums = [] # These are for yaml files!!\n",
    "train_val_test = []\n",
    "for i, pair in enumerate(train_val_pair):\n",
    "    \n",
    "    # Training and validation files separately\n",
    "    train_tmp, val_tmp = train_val_pair[i][0], train_val_pair[i][1]\n",
    "    \n",
    "    # Get the split number of training file\n",
    "    split_num = re.search('_((\\w*)_\\d)', pair[0])\n",
    "    split_nums.append(str(split_num.group(1) + '.yaml')) # For yaml files!!\n",
    "    \n",
    "    train_split_num = split_num.group(2)\n",
    "    for test_tmp in test_files:\n",
    "        # Get the split number of testing file\n",
    "        test_split_num = re.search('_(\\w*)', test_tmp).group(1)\n",
    "        \n",
    "        # If same split number in training, validation and prediction, combine\n",
    "        if train_split_num == test_split_num:\n",
    "            train_val_test.append([train_tmp, val_tmp, test_tmp])\n",
    "            \n",
    "print('Training, validation and testing pairs')\n",
    "print(*train_val_test, sep='\\n')\n",
    "print()\n",
    "\n",
    "print('Total of {} training, validation and testing sets'.format(len(train_val_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d9d870",
   "metadata": {},
   "source": [
    "From the sets above we are going to create the yaml files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6c178529",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training, validation and testing sets are\n",
      "train_split_1_1 \t val_split_1_1 \t test_split_1\n",
      "Yaml file will be named as split_1_1.yaml\n",
      "\n",
      "Training, validation and testing sets are\n",
      "train_split_1_2 \t val_split_1_2 \t test_split_1\n",
      "Yaml file will be named as split_1_2.yaml\n",
      "\n",
      "Training, validation and testing sets are\n",
      "train_split_1_3 \t val_split_1_3 \t test_split_1\n",
      "Yaml file will be named as split_1_3.yaml\n",
      "\n",
      "Training, validation and testing sets are\n",
      "train_split_1_4 \t val_split_1_4 \t test_split_1\n",
      "Yaml file will be named as split_1_4.yaml\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def save_yaml(yaml_str, yaml_path, split):\n",
    "    ''' Save the given string as a yaml file in the given location\n",
    "    '''\n",
    "    \n",
    "    # Make the yaml directory\n",
    "    if not os.path.isdir(yaml_path):\n",
    "        os.mkdir(yaml_path)\n",
    "    \n",
    "    # Write the yaml file\n",
    "    with open(os.path.join(yaml_path, split), 'w') as yaml_file:\n",
    "        yaml = YAML()\n",
    "        code = yaml.load(yaml_str)\n",
    "        yaml.dump(code, yaml_file)\n",
    "    \n",
    "        \n",
    "def create_testing_yaml(test_csv, split):\n",
    "    ''' Make a yaml file for prediction. The base of it is presented above\n",
    "    '''\n",
    "    # The name of the model\n",
    "    # e.g. trained with a yaml file ´split_0_0_smoke.yaml´\n",
    "    #      model saved as `split_0_0_smoke.pth`\n",
    "    model_name = split.split('.')[0] + '.pth'\n",
    "    \n",
    "    yaml_str = '''\\\n",
    "# INITIAL SETTINGS\n",
    "    test_file: {}\n",
    "    model: {}\n",
    "    \n",
    "# TESTING SETTINGS\n",
    "    threshold: {:f}\n",
    "\n",
    "# DEVICE CONFIGS\n",
    "    device_count: {}  \n",
    "    '''.format(test_csv,\n",
    "               model_name,\n",
    "               threshold,\n",
    "               device_count)\n",
    "    \n",
    "    yaml_path = test_yaml_save_path\n",
    "    save_yaml(yaml_str, yaml_path, split)\n",
    "    \n",
    "\n",
    "def create_training_yaml(train_csv, val_csv, split):\n",
    "    ''' Make a yaml file for training. The base of it is presented above\n",
    "    '''\n",
    "    yaml_str = '''\\\n",
    "# INITIAL SETTINGS\n",
    "    train_file: {}\n",
    "    val_file: {}\n",
    "\n",
    "# TRAINING SETTINGS\n",
    "    batch_size: {}\n",
    "    num_workers: {}\n",
    "    epochs: {}\n",
    "    lr: {:f}\n",
    "    weight_decay: {:f}\n",
    "    \n",
    "# VALIDATION SETTINGS\n",
    "    threshold: {:f}\n",
    "\n",
    "# DEVICE CONFIGS\n",
    "    device_count: {}   \n",
    "    '''.format(train_csv,\n",
    "               val_csv,\n",
    "               batch_size,\n",
    "               num_workers, \n",
    "               epochs,\n",
    "               lr,\n",
    "               weight_decay,\n",
    "               threshold,\n",
    "               device_count)\n",
    "    \n",
    "    yaml_path = train_yaml_save_path\n",
    "    save_yaml(yaml_str, yaml_path, split)\n",
    "\n",
    "sets_and_name = list(zip(train_val_test, split_nums))\n",
    "for pair, split in sets_and_name:\n",
    "    train_tmp, val_tmp, test_tmp = pair[0], pair[1], pair[2]\n",
    "    \n",
    "    print('Training, validation and testing sets are')\n",
    "    print(train_tmp.split('.')[0], '\\t', val_tmp.split('.')[0], '\\t', test_tmp.split('.')[0])\n",
    "    print('Yaml file will be named as', split)\n",
    "    print()\n",
    "    \n",
    "    # Training yaml file\n",
    "    create_training_yaml(train_tmp, val_tmp, split)\n",
    "    \n",
    "    # Testing yaml file\n",
    "    create_testing_yaml(test_tmp, split)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9d9f7428",
   "metadata": {},
   "source": [
    "Now all the yaml files for training, validation and testing are created! The training yaml files are located in `/configs/training/train_stratified_smoke/` named as `split_1_1.yaml`, `split0_1.yaml`. `split_1_2.yaml` and `split_1_3.yaml`, and the testing yaml files in `/configs/predicting/predict_stratified_smoke/` named with the same names.\n",
    "\n",
    "<font color=red>**NOTE 1!**</font> It is extremely important that in the test yaml file the model is set with the same name as the yaml file which the model is trained with. E.g. when a model is trained using `split_1_1.yaml`, it will be saved as `split_1_1.pth`. This makes using the repository much easier and simpler. Mind this, if you want to edit the code below.\n",
    "\n",
    "<font color=red>**NOTE 2!**</font> If you are now wondering why the yaml files don't have the csv values --- `train_file`, `val_file` and `test_file` --- in single quotation marks, it's ok. Scripts are able to read and load the values from the yaml files even without those marks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ecg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "af433470baea3cbfb1d2a9219a544bb72a17c8a5091280fdb93be39946c5da4b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
